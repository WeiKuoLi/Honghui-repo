{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqujArrFO7R7zyK5n0TaE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeiKuoLi/simple_test/blob/main/llm_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sUg2A63Oazf",
        "outputId": "c7424837-4c4b-4b1f-aace-ba44062c4687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[1, 0, 0]\n",
            "[-0.34715424  0.1996753 ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "tokenizer = {'A': 0, 'B': 1, 'C': 2}\n",
        "onehot = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "embedding = np.random.randn(2, 3)\n",
        "\n",
        "print(tokenizer['A'])\n",
        "print(onehot[tokenizer['A']])\n",
        "print(embedding @ onehot[tokenizer['A']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding)\n",
        "print(embedding @ onehot[tokenizer['A']])\n",
        "print(embedding @ onehot[tokenizer['B']])\n",
        "print(embedding @ onehot[tokenizer['C']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sig08F3oPM17",
        "outputId": "d0c81c95-174c-41f6-ef44-4c6e0f7ba422"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.34715424 -0.21678652 -0.32696026]\n",
            " [ 0.1996753  -0.38732585  0.01749741]]\n",
            "[-0.34715424  0.1996753 ]\n",
            "[-0.21678652 -0.38732585]\n",
            "[-0.32696026  0.01749741]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'ABCCCB'\n",
        "\n",
        "# get embedding array\n",
        "embedding_array = np.array([embedding @ onehot[tokenizer[char]] for char in text])\n",
        "print(embedding_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MNMepN5Os3b",
        "outputId": "9ef09ac5-04cb-460b-b301-28878297b0d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.34715424  0.1996753 ]\n",
            " [-0.21678652 -0.38732585]\n",
            " [-0.32696026  0.01749741]\n",
            " [-0.32696026  0.01749741]\n",
            " [-0.32696026  0.01749741]\n",
            " [-0.21678652 -0.38732585]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Specify the model name\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Step 1: Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Define the words\n",
        "words = [\"why\", \"what\", \"center\"]\n",
        "\n",
        "# Step 3: Tokenize and extract embeddings\n",
        "embeddings = []\n",
        "for word in words:\n",
        "    # Tokenize the word and get the input IDs\n",
        "    print(\"Current word: \",word)\n",
        "    input_ids = tokenizer(word, return_tensors=\"pt\")[\"input_ids\"]\n",
        "    print(\"Token IDs: \",input_ids)\n",
        "    with torch.no_grad():\n",
        "        # Get the embeddings from the model's input embeddings layer\n",
        "        print(\"embeddings shape: \",model.transformer.wte(input_ids).shape)\n",
        "\n",
        "        # Get the (averaged) embedding for the tokens in the word\n",
        "        token_embedding = model.transformer.wte(input_ids)[0].mean(dim=0)  # [1] to ignore [CLS] token\n",
        "        embeddings.append(token_embedding.numpy())\n",
        "    print('-'*60)\n",
        "\n",
        "# Step 4: Calculate cosine similarity and angles\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def calculate_angle(vec1, vec2):\n",
        "    cos_sim = cosine_similarity(vec1, vec2)\n",
        "    return np.arccos(np.clip(cos_sim, -1.0, 1.0)) * (180 / np.pi)  # Convert radians to degrees\n",
        "\n",
        "for i in range(len(words)):\n",
        "    for j in range(i + 1, len(words)):\n",
        "        angle = calculate_angle(embeddings[i], embeddings[j])\n",
        "        print(f\"Angle between [{words[i]}] and [{words[j]}]: {angle:.2f} degrees\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYrG5gxrO2mm",
        "outputId": "f7de30b5-7c69-445c-e667-ba9cf152e3eb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current word:  why\n",
            "Token IDs:  tensor([[22850]])\n",
            "embeddings shape:  torch.Size([1, 1, 768])\n",
            "------------------------------------------------------------\n",
            "Current word:  what\n",
            "Token IDs:  tensor([[10919]])\n",
            "embeddings shape:  torch.Size([1, 1, 768])\n",
            "------------------------------------------------------------\n",
            "Current word:  center\n",
            "Token IDs:  tensor([[16159]])\n",
            "embeddings shape:  torch.Size([1, 1, 768])\n",
            "------------------------------------------------------------\n",
            "Angle between [why] and [what]: 45.02 degrees\n",
            "Angle between [why] and [center]: 72.07 degrees\n",
            "Angle between [what] and [center]: 72.68 degrees\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tt2C6eTmRIJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}